{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMDyHxkztjps"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "SERPAPI_API_KEY = userdata.get('SERPAPI_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from serpapi import GoogleSearch"
      ],
      "metadata": {
        "id": "lekHB-Wdw-JT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-search-results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqEdovsewXEC",
        "outputId": "a1a373e1-f7e1-4d15-8c73-b05b714f25f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting google-search-results\n",
            "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from google-search-results) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (2025.1.31)\n",
            "Building wheels for collected packages: google-search-results\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32010 sha256=1241e855c008a60294399403040bad14e679bf019c62723d3373b0a5c882f7c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/42/3e/aeb691b02cb7175ec70e2da04b5658d4739d2b41e5f73cd06f\n",
            "Successfully built google-search-results\n",
            "Installing collected packages: google-search-results\n",
            "Successfully installed google-search-results-2.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_search_queries(persona):\n",
        "    name = persona.get(\"name\", \"\")\n",
        "    intro = persona.get(\"intro\", \"\")\n",
        "    # add condition to get the specified fields\n",
        "    # then do data enrichment\n",
        "    # then perform query enhancing, may use an LLM to further enhance the search query\n",
        "    # after that get the n top results from serpapi, and checking which matches the most(match different social media urls here only), if still confusion then on the last few results if image is given then match image, or check other information about the 2nd or other specified information\n",
        "    # validation\n",
        "\n",
        "    queries = [\n",
        "        f'\"{name}\" site:linkedin.com/in',\n",
        "        f'\"{name}\" \"{intro}\" site:linkedin.com/in',\n",
        "    ]\n",
        "\n",
        "    if name and len(name.split()) > 0:\n",
        "        queries.append(f'\"{name.split()[0]}\" \"{intro}\" site:linkedin.com/in')\n",
        "\n",
        "    return queries"
      ],
      "metadata": {
        "id": "yaPI-5COwYl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def search_linkedin_profiles(persona, max_results=5):\n",
        "    queries = build_search_queries(persona)\n",
        "    seen = set()\n",
        "    candidates = []\n",
        "\n",
        "    for query in queries:\n",
        "        params = {\n",
        "            \"engine\": \"google\",\n",
        "            \"q\": query,\n",
        "            \"api_key\": SERPAPI_API_KEY,\n",
        "            \"num\": max_results\n",
        "        }\n",
        "\n",
        "        search = GoogleSearch(params)\n",
        "        results = search.get_dict()\n",
        "        for result in results.get(\"organic_results\", []):\n",
        "            link = result.get(\"link\", \"\")\n",
        "            snippet = result.get(\"snippet\", \"\")\n",
        "            if \"linkedin.com/in/\" in link and link not in seen:\n",
        "                candidates.append({\n",
        "                    \"link\": link,\n",
        "                    \"title\": result.get(\"title\"),\n",
        "                    \"snippet\": snippet\n",
        "                })\n",
        "                seen.add(link)\n",
        "\n",
        "            if len(candidates) >= max_results:\n",
        "                break\n",
        "\n",
        "        if len(candidates) >= max_results:\n",
        "            break\n",
        "\n",
        "    return candidates"
      ],
      "metadata": {
        "id": "lSfRB7dmwoum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "persona = {\n",
        "      \"name\": \"Yaren\",\n",
        "      \"image\": \"\",\n",
        "      \"intro\": \"\",\n",
        "      \"timezone\": \"\",\n",
        "      \"company_industry\": \"\",\n",
        "      \"company_size\": \"0-50 Employees\",\n",
        "      \"social_profile\": []\n",
        "}\n",
        "\n",
        "# intro and timezone, query fetching ke waqt pata lag jayega\n",
        "\n",
        "results = search_linkedin_profiles(persona,20)\n",
        "\n",
        "for i, r in enumerate(results, 1):\n",
        "    print(f\"[{i}] {r['link']}\")\n",
        "    print(f\"Title: {r['title']}\")\n",
        "    print(f\"Snippet: {r['snippet']}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jbt__uB8wsNg",
        "outputId": "1f406225-dfde-4c26-fd1a-d3480b8f68fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] https://www.linkedin.com/in/hakanyarenexecutive\n",
            "Title: Hakan Yaren - Kintetsu World Express\n",
            "Snippet: Hakan Yaren - Kintetsu World Express - About - Executive with leadership roles in Technology, Strategy, M&A, Product Development, and...\n",
            "\n",
            "[2] https://www.linkedin.com/in/yaren-%C3%A7%C4%B1nar\n",
            "Title: Yaren Cinar - Evanston, Illinois, United States\n",
            "Snippet: Yaren Cinar Student at Northwestern University ÜNLÜ & Co The London School of Economics and Political Science (LSE)\n",
            "\n",
            "[3] https://www.linkedin.com/in/yaren-mistacoglu-617568250\n",
            "Title: Yaren Mistacoglu - Strategy Director\n",
            "Snippet: Yaren Mistacoglu. Data Science & Economics @ UC Berkeley. Undergraduate Marketing Association University of California, Berkeley. Berkeley ...\n",
            "\n",
            "[4] https://www.linkedin.com/in/yaren-dogan-985bb7230/\n",
            "Title: Yaren Dogan - Co Director - Hack4Impact UTK\n",
            "Snippet: Hello everyone, my name is Yaren Dogan (yahr-en doe-an). I was born in Istanbul, Turkey, and I moved to the United States when I was 2 years old.\n",
            "\n",
            "[5] https://cy.linkedin.com/in/yaren-fadiloglulari\n",
            "Title: Yaren Fadiloglulari - Content Writer, Copywriter & Journalist\n",
            "Snippet: Yaren Fadiloglulari. Writer. Self Employed Swansea University. Cyprus. 571 followers 500+ connections.\n",
            "\n",
            "[6] https://www.linkedin.com/in/yaren-ay-424b21183\n",
            "Title: Yaren Ay - OneWell Health Care\n",
            "Snippet: Yaren Ay. OneWell Health Care Lynn University. Alexandria, Virginia, United States. 780 followers 500+ connections. See your mutual connections. View ...\n",
            "\n",
            "[7] https://www.linkedin.com/in/yaren-sancak-3b23311b2\n",
            "Title: Yaren Sancak - Key Account Manager - YouParcel\n",
            "Snippet: Yaren Sancak. Graduate of the State University of New York, Binghamton. YouParcel Harrisburg University of Science and Technology. New York City ...\n",
            "\n",
            "[8] https://www.linkedin.com/in/yaren-menemencioglu\n",
            "Title: Yaren Menemencioglu - JPMorgan Chase & Co.\n",
            "Snippet: Yaren Menemencioglu JPMorgan Chase & Co. Başkent Üniversitesi About My specialties include quickly learning new programming languages and skills.\n",
            "\n",
            "[9] https://de.linkedin.com/in/yarenyuzer\n",
            "Title: Yaren Yüzer – Head of Enterprise Sales – Native Teams\n",
            "Snippet: Yaren Yüzer. Head of Enterprise Sales at Native Teams. Native Teams Bilkent University. München, Bayern, Deutschland. 7669 Follower:innen 500+ ...\n",
            "\n",
            "[10] https://www.linkedin.com/in/yaren-%25C3%25B6zd%25C3%25B6l-sugetiren-3ab26a1a\n",
            "Title: Yaren Özdöl Sugetiren - Cluster CFO Rest of Middle East ...\n",
            "Snippet: Yaren Özdöl Sugetiren. CFO @ Signify | Master's in Capital Markets and Stock Exchange, Financial Management, Finance Strategy. Signify Marmara Üniversitesi ...\n",
            "\n",
            "[11] https://www.linkedin.com/in/yaren-yurdagul-9b63011a2\n",
            "Title: Yaren Yurdagul - Graduate Research Assistant\n",
            "Snippet: Yaren Yurdagul · Graduate Food Science Student at the University of Illinois Urbana-Champaign · View mutual connections with Yaren · Welcome back · Experience.\n",
            "\n",
            "[12] https://www.linkedin.com/in/yaren-akcaalan-70a553141\n",
            "Title: Yaren Akcaalan - Keyholder - Kendra Scott\n",
            "Snippet: Yaren Akcaalan. Bachelor of Social Science - Political Science at Kennesaw State University. | Keyholder/leader at Kendra Scott. Kendra Scott ...\n",
            "\n",
            "[13] https://www.linkedin.com/in/yarenbilgekaya\n",
            "Title: Yaren Bilge Kaya - Lecturer - Columbia University\n",
            "Snippet: Experience: Columbia University · Location: New York · 500+ connections on LinkedIn. View Yaren Bilge Kaya's profile on LinkedIn, a professional community ...\n",
            "\n",
            "[14] https://www.linkedin.com/in/yarenbayrak\n",
            "Title: Yaren Türkbeyler - GT İnovasyon\n",
            "Snippet: Human Resources Specialist · Experience: GT İnovasyon · Education: İstanbul Üniversitesi · Location: Istanbul · 500+ connections on LinkedIn. View Yaren ...\n",
            "\n",
            "[15] https://www.linkedin.com/in/yobp\n",
            "Title: Yaren Obando - Deputy Head DLP US - Santander US\n",
            "Snippet: Over that time, Yaren has become a data loss prevention subject matter expert managing and enhancing DLP programs for a multitude of large enterprise-level ...\n",
            "\n",
            "[16] https://www.linkedin.com/in/yaren-aykit-9a5736272\n",
            "Title: Yaren Aykit - Optometric Technician - For Eyes\n",
            "Snippet: Yaren Aykit. Biology Student at Rutgers University - Camden. For Eyes Rutgers University - Camden. Greater Philadelphia.\n",
            "\n",
            "[17] https://www.linkedin.com/in/yaren-usul\n",
            "Title: Yaren Usul - College of Engineering Ambassador\n",
            "Snippet: Yaren Usul · M.S. in Robotics Student at the University of Delaware | Bachelor's in Biomedical Engineering · View mutual connections with Yaren · Welcome back.\n",
            "\n",
            "[18] https://www.linkedin.com/in/yaren-karatas-a6280b208\n",
            "Title: Yaren Karatas - Graduate Research Assistant\n",
            "Snippet: Yaren Karatas. Hydrogeologist. University of Missouri-Kansas City University of Missouri-Kansas City. Kansas City, Missouri, United States. 330 ...\n",
            "\n",
            "[19] https://www.linkedin.com/in/yaren-%C5%9Fahan-881a131bb\n",
            "Title: Yaren Şahan - Export Manager - TANCAN METAL\n",
            "Snippet: Yaren Şahan. Export Manager. TANCAN METAL Selcuk University. Türkiye. 234 followers 203 connections.\n",
            "\n",
            "[20] https://www.linkedin.com/in/buseyarentekin/en\n",
            "Title: Buse Yaren K. - Visiting Student - University of Kentucky\n",
            "Snippet: Buse Yaren K. Artificial Intelligence Researcher at MELIC - Mitsubishi Electric Innovation Center. University of Kentucky Karabuk University. Lexington ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I should create a class with different methods later on to complete a single big macro task"
      ],
      "metadata": {
        "id": "evsoFwkqo6sy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### Step by step implementation\n",
        "def infer_role_from_size_and_intro(company_size: str, intro: str = \"\") -> str:\n",
        "    if intro:\n",
        "        intro_lower = intro.lower()\n",
        "        if any(word in intro_lower for word in [\"founder\", \"co-founder\", \"ceo\", \"started\", \"my company\", \"built\", \"entrepreneur\"]):\n",
        "            return \"Founder\"\n",
        "        elif any(word in intro_lower for word in [\"manager\", \"director\", \"vp\", \"team lead\"]):\n",
        "            return \"Manager\"\n",
        "        elif \"consultant\" in intro_lower:\n",
        "            return \"Consultant\"\n",
        "        elif \"engineer\" in intro_lower:\n",
        "            return \"Engineer\"\n",
        "\n",
        "    # Only fallback to company_size if intro gave no signal\n",
        "    # later we can apply company enrichment as well\n",
        "    # should later on add more keywords and semantic search\n",
        "    if not company_size:\n",
        "        return \"\"\n",
        "\n",
        "    size = company_size.lower()\n",
        "    if \"1\" in size or \"0\" in size:\n",
        "        return \"Founder\"\n",
        "    elif \"11\" in size:\n",
        "        return \"Co-founder\"\n",
        "    elif \"50\" in size:\n",
        "        return \"Team Lead\"\n",
        "    elif \"200\" in size or \"500\" in size:\n",
        "        return \"Manager\"\n",
        "    elif \"1000\" in size or \"+\" in size:\n",
        "        return \"Staff\"  # generic, neutral\n",
        "    return \"\"\n"
      ],
      "metadata": {
        "id": "RFiEsT4rwvCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "infer_role_from_size_and_intro(\"5000\",\"entrepreneur\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bgoyLjDWozuL",
        "outputId": "589d3a13-78db-4f3c-96f4-9da7cc1545be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Founder'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Later on add more different accounts support"
      ],
      "metadata": {
        "id": "d6SE3Z7uqJSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add more different scrapers for different platforms to get more additional information possible"
      ],
      "metadata": {
        "id": "vagv55ftyPlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_social_usernames(social_profiles):\n",
        "    usernames_dict = {}\n",
        "\n",
        "    # Patterns for different social media platforms\n",
        "    patterns = {\n",
        "        'twitter': r'(?:twitter\\.com|x\\.com)/(@?[\\w\\d\\-_]+)',\n",
        "        'github': r'github\\.com/([\\w\\d\\-_]+)',\n",
        "        'bluesky': r'(?:bsky\\.app|bsky\\.social)/profile/([\\w\\d\\-_\\.]+)',\n",
        "        'instagram': r'instagram\\.com/(?!p/)(@?[\\w\\d\\._]+)',\n",
        "        'facebook': r'facebook\\.com/(?!(?:pages|groups|events)/)([^/\\?]+)',\n",
        "        'linkedin': r'linkedin\\.com/in/([\\w\\d\\-_]+)',\n",
        "        'youtube': r'youtube\\.com/(?:c/|channel/|user/|@)([\\w\\d\\-_]+)',\n",
        "        'tiktok': r'tiktok\\.com/(@[\\w\\d\\._]+)',\n",
        "        'mastodon': r'([\\w\\d\\-_\\.]+)@([\\w\\d\\-_\\.]+)',  # username@instance.domain\n",
        "        'threads': r'threads\\.net/(@?[\\w\\d\\._]+)'\n",
        "    }\n",
        "\n",
        "    for url in social_profiles:\n",
        "        url = url.strip()\n",
        "\n",
        "        for platform, pattern in patterns.items():\n",
        "            match = re.search(pattern, url)\n",
        "            if match:\n",
        "                username = match.group(1)\n",
        "\n",
        "                # Format usernames properly\n",
        "                if platform == 'twitter' and not username.startswith('@'):\n",
        "                    username = f'@{username}'\n",
        "                elif platform == 'tiktok' and not username.startswith('@'):\n",
        "                    username = f'@{username}'\n",
        "                elif platform == 'instagram' and not username.startswith('@'):\n",
        "                    username = f'@{username}'\n",
        "                elif platform == 'bluesky' and not username.startswith('@'):\n",
        "                    username = f'@{username}'\n",
        "\n",
        "                # Add to results dictionary\n",
        "                if platform not in usernames_dict:\n",
        "                    usernames_dict[platform] = []\n",
        "                usernames_dict[platform].append(username)\n",
        "                break\n",
        "\n",
        "    return usernames_dict"
      ],
      "metadata": {
        "id": "CJZBdAY4pJ6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "profiles = [\n",
        "        \"https://twitter.com/rajavijayach\"\n",
        "    ]\n",
        "\n",
        "results = extract_social_usernames(profiles)\n",
        "for platform, users in results.items():\n",
        "        print(f\"{platform}: {users}\")"
      ],
      "metadata": {
        "id": "XHlAUcrTqQnO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13348384-7412-4828-8faf-a25f8e58bf12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "twitter: ['@rajavijayach']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Name Variants"
      ],
      "metadata": {
        "id": "6eon-PnVzD_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nameparser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqvFhqlnzPJO",
        "outputId": "f646d2ad-1fac-45c7-9d8c-c17bde4d0476"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nameparser\n",
            "  Downloading nameparser-1.1.3-py2.py3-none-any.whl.metadata (6.1 kB)\n",
            "Downloading nameparser-1.1.3-py2.py3-none-any.whl (24 kB)\n",
            "Installing collected packages: nameparser\n",
            "Successfully installed nameparser-1.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nameparser import HumanName\n",
        "from typing import Dict, List, Set, Tuple, Any, Optional\n",
        "\n",
        "def generate_name_variants(name: str) -> List[str]:\n",
        "    \"\"\"Generate multiple name format variants from a full name.\"\"\"\n",
        "    if not name:\n",
        "        return []\n",
        "\n",
        "    variants = set()\n",
        "    parsed = HumanName(name)\n",
        "\n",
        "    # Add first name\n",
        "    if parsed.first:\n",
        "        variants.add(parsed.first)\n",
        "\n",
        "        # Add first + last name combinations\n",
        "        if parsed.last:\n",
        "            variants.add(f\"{parsed.first} {parsed.last}\")\n",
        "            variants.add(f\"{parsed.first[0]}. {parsed.last}\")\n",
        "\n",
        "            # Add middle initial variants if available\n",
        "            if parsed.middle:\n",
        "                variants.add(f\"{parsed.first} {parsed.middle[0]}. {parsed.last}\")\n",
        "                variants.add(f\"{parsed.first} {parsed.middle} {parsed.last}\")\n",
        "\n",
        "    # Add full name and nickname variants\n",
        "    variants.add(name)\n",
        "    if parsed.nickname:\n",
        "        variants.add(parsed.nickname)\n",
        "        if parsed.last:\n",
        "            variants.add(f\"{parsed.nickname} {parsed.last}\")\n",
        "\n",
        "    return list(variants)"
      ],
      "metadata": {
        "id": "vXKYil5-xgdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_name_variants(\"dArshan Surendra Thakare\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nmhJcV0zNtA",
        "outputId": "9bf9019d-069e-4cac-ea72-5d68a987be94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dArshan Thakare',\n",
              " 'd. Thakare',\n",
              " 'dArshan',\n",
              " 'dArshan S. Thakare',\n",
              " 'dArshan Surendra Thakare']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enriching the different user personas"
      ],
      "metadata": {
        "id": "z5zdDI7jDqTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Work on this again indepth"
      ],
      "metadata": {
        "id": "LFa5pgSAEId7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def enrich_persona_with_api(persona: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Optional: Try enriching the persona using PeopleDataLabs or Clearbit\n",
        "    if name-only data is available.\n",
        "    Returns the enriched persona or the same if not enriched.\n",
        "    \"\"\"\n",
        "    # Defensive programming\n",
        "    if not persona or not isinstance(persona, dict):\n",
        "        return persona\n",
        "\n",
        "    name = persona.get(\"name\", \"\")\n",
        "    if not name or len(name.split()) < 1:\n",
        "        return persona  # not enough info to enrich\n",
        "\n",
        "    # Example enrichment\n",
        "    # result = people_data_labs_enrich(name=name)\n",
        "    # if result:\n",
        "    #     persona[\"company_industry\"] = result.get(\"industry\")\n",
        "    #     persona[\"intro\"] = result.get(\"job_title\")\n",
        "    #     persona[\"location\"] = result.get(\"location\")\n",
        "\n",
        "    return persona"
      ],
      "metadata": {
        "id": "-f5H-3rEztQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assigning Scores"
      ],
      "metadata": {
        "id": "E4ueiDltEP9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def score_query(query: str, weights: Dict[str, float]) -> float:\n",
        "    \"\"\"\n",
        "    Score a search query based on the weights of its components.\n",
        "    Applies some penalties and bonuses based on query structure.\n",
        "    \"\"\"\n",
        "    score = sum(weights.values())\n",
        "\n",
        "    # Bonus for queries with multiple high-value components\n",
        "    if len(weights) >= 3:\n",
        "        score *= 1.2\n",
        "\n",
        "    # Penalty for very long queries\n",
        "    if len(query.split()) > 10:\n",
        "        score *= 0.9\n",
        "\n",
        "    return score"
      ],
      "metadata": {
        "id": "DsXqNVuoD1B2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Major function"
      ],
      "metadata": {
        "id": "zfclGiagEq1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_search_queries(persona: Dict[str, Any]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Generate ranked search queries based on a person's information.\n",
        "    Returns a list of search queries ordered by relevance.\n",
        "    \"\"\"\n",
        "    # Input validation\n",
        "    if not persona or not isinstance(persona, dict):\n",
        "        return []\n",
        "\n",
        "    # Enrich and extract persona data\n",
        "    persona = enrich_persona_with_api(persona)\n",
        "    name = persona.get(\"name\", \"\")\n",
        "    intro = persona.get(\"intro\", \"\")\n",
        "    company = persona.get(\"company_industry\", \"\")\n",
        "    company_size = persona.get(\"company_size\", \"\")\n",
        "    social_profiles = persona.get(\"social_profile\", [])\n",
        "    location = persona.get(\"location\", \"\")\n",
        "\n",
        "    if not name:  # Name is required\n",
        "        return []\n",
        "\n",
        "    queries = []\n",
        "\n",
        "    # Build base name variants\n",
        "    name_variants = generate_name_variants(name)\n",
        "\n",
        "    # Get social usernames\n",
        "    social_data = extract_social_usernames(social_profiles)\n",
        "    social_usernames = []\n",
        "    for platform_usernames in social_data.values():\n",
        "        social_usernames.extend(platform_usernames)\n",
        "\n",
        "    # Infer likely role based on company size\n",
        "    inferred_role = infer_role_from_size_and_intro(company_size,intro)\n",
        "\n",
        "    # Generate queries with various combinations\n",
        "    for variant in name_variants:\n",
        "        # Basic name-only query\n",
        "        base = f'\"{variant}\" site:linkedin.com/in'\n",
        "        queries.append((base, {\"name\": 1.0}))\n",
        "\n",
        "        # Name + job title/intro\n",
        "        if intro:\n",
        "            queries.append((f'\"{variant}\" \"{intro}\" site:linkedin.com/in',\n",
        "                           {\"name\": 1.0, \"intro\": 0.8}))\n",
        "\n",
        "        # Name + company/industry\n",
        "        if company:\n",
        "            queries.append((f'\"{variant}\" \"{company}\" site:linkedin.com/in',\n",
        "                           {\"name\": 1.0, \"company\": 0.7}))\n",
        "            # Name + company + intro for higher specificity\n",
        "            if intro:\n",
        "                queries.append((f'\"{variant}\" \"{company}\" \"{intro}\" site:linkedin.com/in',\n",
        "                               {\"name\": 1.0, \"company\": 0.7, \"intro\": 0.8}))\n",
        "\n",
        "        # Name + inferred role\n",
        "        if inferred_role:\n",
        "            queries.append((f'\"{variant}\" \"{inferred_role}\" site:linkedin.com/in',\n",
        "                           {\"name\": 1.0, \"role\": 0.6}))\n",
        "            # Name + inferred role + company\n",
        "            if company:\n",
        "                queries.append((f'\"{variant}\" \"{inferred_role}\" \"{company}\" site:linkedin.com/in',\n",
        "                               {\"name\": 1.0, \"role\": 0.6, \"company\": 0.7}))\n",
        "\n",
        "        # Name + location for local professionals\n",
        "        if location:\n",
        "            queries.append((f'\"{variant}\" \"{location}\" site:linkedin.com/in',\n",
        "                           {\"name\": 1.0, \"location\": 0.5}))\n",
        "\n",
        "        # Fallback generic role queries\n",
        "        common_roles = [\"Founder\", \"CEO\", \"CTO\", \"Director\", \"Manager\", \"Lead\"]\n",
        "        if not intro and not inferred_role:\n",
        "            for role in common_roles:\n",
        "                queries.append((f'\"{variant}\" \"{role}\" site:linkedin.com/in',\n",
        "                               {\"name\": 1.0, \"fallback\": 0.4}))\n",
        "\n",
        "    # Social handle-based queries\n",
        "    for handle in social_usernames:\n",
        "        queries.append((f'\"{handle}\" site:linkedin.com/in', {\"social\": 0.9}))\n",
        "\n",
        "        # Try to combine social handles with name for better matches\n",
        "        if name_variants:\n",
        "            primary_name = name_variants[0]\n",
        "            queries.append((f'\"{handle}\" \"{primary_name}\" site:linkedin.com/in',\n",
        "                           {\"social\": 0.9, \"name\": 1.0}))\n",
        "\n",
        "    # Deduplicate and rank queries\n",
        "    seen = set()\n",
        "    ranked_queries = []\n",
        "\n",
        "    for query, weights in queries:\n",
        "        if query not in seen:\n",
        "            ranked_queries.append((query, score_query(query, weights)))\n",
        "            seen.add(query)\n",
        "\n",
        "    # Sort by score in descending order\n",
        "    ranked_queries.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Return only the query strings, not their scores\n",
        "    return [query for query, _ in ranked_queries]"
      ],
      "metadata": {
        "id": "wBEmr3fmEgvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "persona = {\n",
        "    \"name\": \"Samantha Carter\",\n",
        "    \"intro\": \"Head of Product at Astrogate — building AI systems for enterprise automation\",\n",
        "    \"company_industry\": \"Artificial Intelligence\",\n",
        "    \"company_size\": \"51–200 Employees\",\n",
        "    \"timezone\": \"America/New_York\",\n",
        "    \"location\": \"Brooklyn, NY\",\n",
        "    \"social_profile\": [\n",
        "        \"https://twitter.com/samcarter_ai\",\n",
        "        \"https://github.com/samcarter\"\n",
        "    ],\n",
        "    \"image\": \"https://images.generated.photos/samantha_carter.jpg\"\n",
        "}\n",
        "\n",
        "# need to work on company_industry and company_size finding\n"
      ],
      "metadata": {
        "id": "PubX5GWHE-aS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_search_queries(persona)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8QcZeLVF30e",
        "outputId": "b6d3b18a-a66e-46df-e3b7-8086ca362af8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\"Samantha Carter\" \"Founder\" \"Artificial Intelligence\" site:linkedin.com/in',\n",
              " '\"S. Carter\" \"Founder\" \"Artificial Intelligence\" site:linkedin.com/in',\n",
              " '\"Samantha\" \"Founder\" \"Artificial Intelligence\" site:linkedin.com/in',\n",
              " '\"Samantha Carter\" \"Artificial Intelligence\" \"Head of Product at Astrogate — building AI systems for enterprise automation\" site:linkedin.com/in',\n",
              " '\"S. Carter\" \"Artificial Intelligence\" \"Head of Product at Astrogate — building AI systems for enterprise automation\" site:linkedin.com/in',\n",
              " '\"Samantha\" \"Artificial Intelligence\" \"Head of Product at Astrogate — building AI systems for enterprise automation\" site:linkedin.com/in',\n",
              " '\"@samcarter_ai\" \"Samantha Carter\" site:linkedin.com/in',\n",
              " '\"samcarter\" \"Samantha Carter\" site:linkedin.com/in',\n",
              " '\"Samantha Carter\" \"Artificial Intelligence\" site:linkedin.com/in',\n",
              " '\"S. Carter\" \"Artificial Intelligence\" site:linkedin.com/in',\n",
              " '\"Samantha\" \"Artificial Intelligence\" site:linkedin.com/in',\n",
              " '\"Samantha Carter\" \"Head of Product at Astrogate — building AI systems for enterprise automation\" site:linkedin.com/in',\n",
              " '\"S. Carter\" \"Head of Product at Astrogate — building AI systems for enterprise automation\" site:linkedin.com/in',\n",
              " '\"Samantha\" \"Head of Product at Astrogate — building AI systems for enterprise automation\" site:linkedin.com/in',\n",
              " '\"Samantha Carter\" \"Founder\" site:linkedin.com/in',\n",
              " '\"S. Carter\" \"Founder\" site:linkedin.com/in',\n",
              " '\"Samantha\" \"Founder\" site:linkedin.com/in',\n",
              " '\"Samantha Carter\" \"Brooklyn, NY\" site:linkedin.com/in',\n",
              " '\"S. Carter\" \"Brooklyn, NY\" site:linkedin.com/in',\n",
              " '\"Samantha\" \"Brooklyn, NY\" site:linkedin.com/in',\n",
              " '\"Samantha Carter\" site:linkedin.com/in',\n",
              " '\"S. Carter\" site:linkedin.com/in',\n",
              " '\"Samantha\" site:linkedin.com/in',\n",
              " '\"@samcarter_ai\" site:linkedin.com/in',\n",
              " '\"samcarter\" site:linkedin.com/in']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## So generating advanced search query is done."
      ],
      "metadata": {
        "id": "gL-pW0snGckS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now first let's just see what all things to add\n",
        "\n",
        "1. People enrichment if we find the profile at this stage, then it's amazing\n",
        "2. We will get results: then from those results. rank those on the basis of cosine similarity with the given persona. {{This is also enough}}\n",
        "3. Then after this some targeted tests like image matching and more, addition\n",
        "\n",
        "Build this much till today.\n",
        "\n",
        "4. Making the code better by dividing into multiple small modules\n",
        "5. Validation engine to validate\n",
        "6. API creation and deployment\n",
        "7. Build UI/UX and done"
      ],
      "metadata": {
        "id": "DHNGaAJpU653"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section I am writing the code for image matching, so once we get some highly specific queries, if image is provided then we will directly match the images"
      ],
      "metadata": {
        "id": "3Lc1DP3-iURC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAEzTDMomx3T",
        "outputId": "9b2e8728-2d4d-4f24-fd44-72bdc58aa734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-8nnzt2eb\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-8nnzt2eb\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "def get_clip_embedding_from_url(image_url):\n",
        "    try:\n",
        "        # Add timeout and headers to mimic a browser request\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "        response = requests.get(image_url, timeout=10, headers=headers)\n",
        "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "\n",
        "        # Check content type to verify it's an image\n",
        "        content_type = response.headers.get('Content-Type', '')\n",
        "        if not content_type.startswith('image/'):\n",
        "            print(f\"Warning: URL doesn't return image content type. Got: {content_type}\")\n",
        "\n",
        "        # Debug: save first few bytes to see what we're getting\n",
        "        content_preview = response.content[:30]\n",
        "        print(f\"Content preview (first 30 bytes): {content_preview}\")\n",
        "\n",
        "        # Try to open the image\n",
        "        image = Image.open(BytesIO(response.content))\n",
        "        print(f\"Image format: {image.format}, Size: {image.size}, Mode: {image.mode}\")\n",
        "\n",
        "        # Convert to RGB if needed\n",
        "        if image.mode != \"RGB\":\n",
        "            image = image.convert(\"RGB\")\n",
        "\n",
        "        # Process and get embedding\n",
        "        processed_image = preprocess(image).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            embedding = model.encode_image(processed_image)\n",
        "            embedding /= embedding.norm(dim=-1, keepdim=True)\n",
        "        return embedding\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Request error: {e}\")\n",
        "    except PIL.UnidentifiedImageError as e:\n",
        "        print(f\"Error identifying image: {e}\")\n",
        "        # Try to print the first few bytes to debug\n",
        "        if 'response' in locals() and hasattr(response, 'content'):\n",
        "            print(f\"First 50 bytes: {response.content[:50]}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "1EmrQSD_F_fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_image_similarity_clip(url1, url2):\n",
        "    print(f\"Processing first image from: {url1}\")\n",
        "    emb1 = get_clip_embedding_from_url(url1)\n",
        "\n",
        "    print(f\"Processing second image from: {url2}\")\n",
        "    emb2 = get_clip_embedding_from_url(url2)\n",
        "\n",
        "    if emb1 is None:\n",
        "        return {\"score\": 0.0, \"reason\": f\"First image load or encoding failed\"}\n",
        "    if emb2 is None:\n",
        "        return {\"score\": 0.0, \"reason\": f\"Second image load or encoding failed\"}\n",
        "\n",
        "    similarity = (emb1 @ emb2.T).item()  # cosine similarity\n",
        "    return {\n",
        "        \"score\": round(similarity, 3),  # usually 0.0 to 1.0\n",
        "        \"reason\": \"OK\"\n",
        "    }"
      ],
      "metadata": {
        "id": "aCanfB-4m2N9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_image_similarity_clip(\"https://upload.wikimedia.org/wikipedia/commons/9/99/Black_square.jpg\",\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/99/Black_square.jpg/800px-Black_square.jpg\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tD7CiadrX5J",
        "outputId": "926e0060-3169-4928-ad3e-982751fc0852"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing first image from: https://upload.wikimedia.org/wikipedia/commons/9/99/Black_square.jpg\n",
            "Content preview (first 30 bytes): b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x0b\\x07\\x08\\t\\x08'\n",
            "Image format: JPEG, Size: (360, 360), Mode: RGB\n",
            "Processing second image from: https://upload.wikimedia.org/wikipedia/commons/thumb/9/99/Black_square.jpg/800px-Black_square.jpg\n",
            "Content preview (first 30 bytes): b'\\xff\\xd8\\xff\\xdb\\x00C\\x00\\x04\\x03\\x03\\x04\\x03\\x03\\x04\\x04\\x03\\x04\\x05\\x04\\x04\\x05\\x06\\n\\x07\\x06\\x06\\x06\\x06\\r\\t'\n",
            "Image format: JPEG, Size: (800, 800), Mode: RGB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.985, 'reason': 'OK'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compare_image_similarity_clip(\"https://upload.wikimedia.org/wikipedia/commons/3/3f/Fronalpstock_big.jpg\",\"https://upload.wikimedia.org/wikipedia/commons/9/99/Black_square.jpg\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGP9SaBquTQf",
        "outputId": "e6d22823-9317-48b5-995a-56fc42780b54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing first image from: https://upload.wikimedia.org/wikipedia/commons/3/3f/Fronalpstock_big.jpg\n",
            "Content preview (first 30 bytes): b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x02\\x01\\x01,\\x01,\\x00\\x00\\xff\\xe1\\x13DExif\\x00\\x00'\n",
            "Image format: JPEG, Size: (10109, 4542), Mode: RGB\n",
            "Processing second image from: https://upload.wikimedia.org/wikipedia/commons/9/99/Black_square.jpg\n",
            "Content preview (first 30 bytes): b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x0b\\x07\\x08\\t\\x08'\n",
            "Image format: JPEG, Size: (360, 360), Mode: RGB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.475, 'reason': 'OK'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Social Media URL traversal\n",
        "\n",
        "1. Just fetch the information of different fields required and store in the array\n",
        "2. Then create a good string with all the information\n",
        "3. And send this to the gemini api and get back a more informative persona"
      ],
      "metadata": {
        "id": "2nZbDfJuwOXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def extract_github_info(username):\n",
        "    r = requests.get(f\"https://api.github.com/users/{username}\")\n",
        "    if r.status_code == 200:\n",
        "        data = r.json()\n",
        "        # return {\n",
        "        #     \"name\": data.get(\"name\"),\n",
        "        #     \"bio\": data.get(\"bio\"),\n",
        "        #     \"location\": data.get(\"location\"),\n",
        "        #     \"email\": data.get(\"email\"),\n",
        "        #     \"twitter_username\": data.get(\"twitter_username\")\n",
        "        # }\n",
        "        return data\n",
        "    return {}\n",
        "\n",
        "# I want name,company,location,email,bio,twitter_username\n",
        "# sometimes people mention their linkedin on their different social threads as well\n"
      ],
      "metadata": {
        "id": "F1BpgKK-06dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_github_info(\"Samia35-2973\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnWEWJLW2BDD",
        "outputId": "0b41e4e2-434c-4376-cb2d-48c2fe49de46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'login': 'Samia35-2973',\n",
              " 'id': 71200431,\n",
              " 'node_id': 'MDQ6VXNlcjcxMjAwNDMx',\n",
              " 'avatar_url': 'https://avatars.githubusercontent.com/u/71200431?v=4',\n",
              " 'gravatar_id': '',\n",
              " 'url': 'https://api.github.com/users/Samia35-2973',\n",
              " 'html_url': 'https://github.com/Samia35-2973',\n",
              " 'followers_url': 'https://api.github.com/users/Samia35-2973/followers',\n",
              " 'following_url': 'https://api.github.com/users/Samia35-2973/following{/other_user}',\n",
              " 'gists_url': 'https://api.github.com/users/Samia35-2973/gists{/gist_id}',\n",
              " 'starred_url': 'https://api.github.com/users/Samia35-2973/starred{/owner}{/repo}',\n",
              " 'subscriptions_url': 'https://api.github.com/users/Samia35-2973/subscriptions',\n",
              " 'organizations_url': 'https://api.github.com/users/Samia35-2973/orgs',\n",
              " 'repos_url': 'https://api.github.com/users/Samia35-2973/repos',\n",
              " 'events_url': 'https://api.github.com/users/Samia35-2973/events{/privacy}',\n",
              " 'received_events_url': 'https://api.github.com/users/Samia35-2973/received_events',\n",
              " 'type': 'User',\n",
              " 'user_view_type': 'public',\n",
              " 'site_admin': False,\n",
              " 'name': 'Samia Haque Tisha',\n",
              " 'company': None,\n",
              " 'blog': '',\n",
              " 'location': 'Dania, Jatrabari, Dhaka',\n",
              " 'email': None,\n",
              " 'hireable': True,\n",
              " 'bio': 'Data analyst and AI/ML enthusiast, blending problem-solving passion with expertise in crafting intelligent solutions.',\n",
              " 'twitter_username': None,\n",
              " 'public_repos': 33,\n",
              " 'public_gists': 0,\n",
              " 'followers': 26,\n",
              " 'following': 23,\n",
              " 'created_at': '2020-09-13T02:00:39Z',\n",
              " 'updated_at': '2025-03-29T18:28:25Z'}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nitter-scraper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVovIRt14jJo",
        "outputId": "4dfa7d34-369a-433c-8fc7-a93433c8a210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nitter-scraper in /usr/local/lib/python3.11/dist-packages (0.5.0)\n",
            "Requirement already satisfied: docker<5.0.0,>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from nitter-scraper) (4.4.4)\n",
            "Requirement already satisfied: jinja2<3.0.0,>=2.11.2 in /usr/local/lib/python3.11/dist-packages (from nitter-scraper) (2.11.3)\n",
            "Requirement already satisfied: loguru<0.6.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from nitter-scraper) (0.5.3)\n",
            "Requirement already satisfied: pendulum<3.0.0,>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from nitter-scraper) (2.1.2)\n",
            "Requirement already satisfied: pydantic<2.0.0,>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from nitter-scraper) (1.10.21)\n",
            "Requirement already satisfied: requests-html<0.11.0,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from nitter-scraper) (0.10.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker<5.0.0,>=4.3.1->nitter-scraper) (1.17.0)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from docker<5.0.0,>=4.3.1->nitter-scraper) (1.8.0)\n",
            "Requirement already satisfied: requests!=2.18.0,>=2.14.2 in /usr/local/lib/python3.11/dist-packages (from docker<5.0.0,>=4.3.1->nitter-scraper) (2.32.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.11/dist-packages (from jinja2<3.0.0,>=2.11.2->nitter-scraper) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil<3.0,>=2.6 in /usr/local/lib/python3.11/dist-packages (from pendulum<3.0.0,>=2.1.2->nitter-scraper) (2.8.2)\n",
            "Requirement already satisfied: pytzdata>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pendulum<3.0.0,>=2.1.2->nitter-scraper) (2020.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.0.0,>=1.6.1->nitter-scraper) (4.13.1)\n",
            "Requirement already satisfied: pyquery in /usr/local/lib/python3.11/dist-packages (from requests-html<0.11.0,>=0.10.0->nitter-scraper) (2.0.1)\n",
            "Requirement already satisfied: fake-useragent in /usr/local/lib/python3.11/dist-packages (from requests-html<0.11.0,>=0.10.0->nitter-scraper) (2.1.0)\n",
            "Requirement already satisfied: parse in /usr/local/lib/python3.11/dist-packages (from requests-html<0.11.0,>=0.10.0->nitter-scraper) (1.20.2)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.11/dist-packages (from requests-html<0.11.0,>=0.10.0->nitter-scraper) (0.0.2)\n",
            "Requirement already satisfied: w3lib in /usr/local/lib/python3.11/dist-packages (from requests-html<0.11.0,>=0.10.0->nitter-scraper) (2.3.1)\n",
            "Requirement already satisfied: pyppeteer>=0.0.14 in /usr/local/lib/python3.11/dist-packages (from requests-html<0.11.0,>=0.10.0->nitter-scraper) (2.0.0)\n",
            "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->nitter-scraper) (1.4.4)\n",
            "Requirement already satisfied: certifi>=2023 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->nitter-scraper) (2025.1.31)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->nitter-scraper) (8.6.1)\n",
            "Requirement already satisfied: pyee<12.0.0,>=11.0.0 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->nitter-scraper) (11.1.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->nitter-scraper) (4.67.1)\n",
            "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->nitter-scraper) (1.26.20)\n",
            "Requirement already satisfied: websockets<11.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->nitter-scraper) (10.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests!=2.18.0,>=2.14.2->docker<5.0.0,>=4.3.1->nitter-scraper) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests!=2.18.0,>=2.14.2->docker<5.0.0,>=4.3.1->nitter-scraper) (3.10)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from bs4->requests-html<0.11.0,>=0.10.0->nitter-scraper) (4.13.3)\n",
            "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.11/dist-packages (from pyquery->requests-html<0.11.0,>=0.10.0->nitter-scraper) (5.3.1)\n",
            "Requirement already satisfied: cssselect>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from pyquery->requests-html<0.11.0,>=0.10.0->nitter-scraper) (1.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->nitter-scraper) (3.21.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4->requests-html<0.11.0,>=0.10.0->nitter-scraper) (2.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nitter_scraper import NitterScraper\n",
        "\n",
        "def extract_twitter_info(handle):\n",
        "    try:\n",
        "        scraper = NitterScraper()\n",
        "        user_profile = scraper.get_profile(handle)\n",
        "\n",
        "        return {\n",
        "            \"username\": handle,\n",
        "            \"displayname\": user_profile.display_name,\n",
        "            \"bio\": user_profile.bio,\n",
        "            \"location\": user_profile.location,\n",
        "            \"followers\": user_profile.followers,\n",
        "            \"verified\": user_profile.is_verified\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Error extracting Twitter info: {str(e)}\"}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "37roruMx2Ico",
        "outputId": "c783a7f2-a37e-4c3f-db2c-73d56e1e0d2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'soft_unicode' from 'markupsafe' (/usr/local/lib/python3.11/dist-packages/markupsafe/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-296cf98525cd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnitter_scraper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNitterScraper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_twitter_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mscraper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNitterScraper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nitter_scraper/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnitter_scraper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnitter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNitterScraper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnitter_scraper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_profile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnitter_scraper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_tweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"get_profile\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"get_tweets\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"NitterScraper\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nitter_scraper/nitter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdocker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDockerClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdocker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mContainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjinja2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFileSystemLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mloguru\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpydantic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseModel\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mBase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jinja2/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbccache\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFileSystemBytecodeCache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbccache\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMemcachedBytecodeCache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0menvironment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0menvironment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTemplate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTemplateAssertionError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jinja2/environment.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcompiler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCodeGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcompiler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBLOCK_END_STRING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBLOCK_START_STRING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCOMMENT_END_STRING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jinja2/defaults.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# -*- coding: utf-8 -*-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_compat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrange_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFILTERS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mDEFAULT_FILTERS\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTESTS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mDEFAULT_TESTS\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCycler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jinja2/filters.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmarkupsafe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mescape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmarkupsafe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMarkup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmarkupsafe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msoft_unicode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_compat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'soft_unicode' from 'markupsafe' (/usr/local/lib/python3.11/dist-packages/markupsafe/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extract_twitter_info(\"imVkohli\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFnuaPiX4n84",
        "outputId": "11ee7725-d42f-43ea-a810-1ffe7c41a597"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:snscrape.base:Error retrieving https://twitter.com/imVkohli: SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /imVkohli (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1016)')))\"))\n",
            "CRITICAL:snscrape.base:4 requests to https://twitter.com/imVkohli failed, giving up.\n",
            "CRITICAL:snscrape.base:Errors: SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /imVkohli (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1016)')))\")), SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /imVkohli (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1016)')))\")), SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /imVkohli (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1016)')))\")), SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /imVkohli (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1016)')))\"))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'error': 'Error extracting Twitter info: 4 requests to https://twitter.com/imVkohli failed, giving up.'}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install async_timeout"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsSX8loK-Hoy",
        "outputId": "915893e9-e5d1-44f3-b1b0-544524ad5a82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting async_timeout\n",
            "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
            "Installing collected packages: async_timeout\n",
            "Successfully installed async_timeout-5.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade -e git+https://github.com/twintproject/twint.git@origin/master#egg=twint"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vci7TFx_5Enp",
        "outputId": "1f0f0296-8c92-4b03-d2ee-c37b9c7bedb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining twint from git+https://github.com/twintproject/twint.git@origin/master#egg=twint\n",
            "  Cloning https://github.com/twintproject/twint.git (to revision origin/master) to ./src/twint\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/twintproject/twint.git /content/src/twint\n",
            "\u001b[33m  WARNING: Did not find branch or tag 'origin/master', assuming revision or ref.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Running command git checkout -q origin/master\n",
            "  Resolved https://github.com/twintproject/twint.git to commit origin/master\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from twint) (3.11.15)\n",
            "Collecting aiodns (from twint)\n",
            "  Downloading aiodns-3.2.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from twint) (4.13.3)\n",
            "Collecting cchardet (from twint)\n",
            "  Downloading cchardet-2.1.7.tar.gz (653 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m653.6/653.6 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dataclasses (from twint)\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting elasticsearch (from twint)\n",
            "  Downloading elasticsearch-8.17.2-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: pysocks in /usr/local/lib/python3.11/dist-packages (from twint) (1.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from twint) (2.2.2)\n",
            "Collecting aiohttp_socks (from twint)\n",
            "  Downloading aiohttp_socks-0.10.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting schedule (from twint)\n",
            "  Downloading schedule-1.2.2-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: geopy in /usr/local/lib/python3.11/dist-packages (from twint) (2.4.1)\n",
            "Requirement already satisfied: fake-useragent in /usr/local/lib/python3.11/dist-packages (from twint) (2.1.0)\n",
            "Collecting googletransx (from twint)\n",
            "  Downloading googletransx-2.4.2.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pycares>=4.0.0 (from aiodns->twint)\n",
            "  Downloading pycares-4.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->twint) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->twint) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->twint) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->twint) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->twint) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->twint) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->twint) (1.18.3)\n",
            "Collecting python-socks<3.0.0,>=2.4.3 (from python-socks[asyncio]<3.0.0,>=2.4.3->aiohttp_socks->twint)\n",
            "  Downloading python_socks-2.7.1-py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->twint) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->twint) (4.13.1)\n",
            "Collecting elastic-transport<9,>=8.15.1 (from elasticsearch->twint)\n",
            "  Downloading elastic_transport-8.17.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: geographiclib<3,>=1.52 in /usr/local/lib/python3.11/dist-packages (from geopy->twint) (2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from googletransx->twint) (2.32.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->twint) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->twint) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->twint) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->twint) (2025.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from elastic-transport<9,>=8.15.1->elasticsearch->twint) (1.26.20)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from elastic-transport<9,>=8.15.1->elasticsearch->twint) (2025.1.31)\n",
            "Requirement already satisfied: cffi>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from pycares>=4.0.0->aiodns->twint) (1.17.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->twint) (1.17.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp->twint) (3.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->googletransx->twint) (3.4.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.5.0->pycares>=4.0.0->aiodns->twint) (2.22)\n",
            "Downloading aiodns-3.2.0-py3-none-any.whl (5.7 kB)\n",
            "Downloading aiohttp_socks-0.10.1-py3-none-any.whl (10 kB)\n",
            "Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Downloading elasticsearch-8.17.2-py3-none-any.whl (717 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.0/718.0 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading schedule-1.2.2-py3-none-any.whl (12 kB)\n",
            "Downloading elastic_transport-8.17.1-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycares-4.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.3/289.3 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_socks-2.7.1-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.0/55.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: cchardet, googletransx\n",
            "  Building wheel for cchardet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cchardet: filename=cchardet-2.1.7-cp311-cp311-linux_x86_64.whl size=295278 sha256=ae70c2fdcc600af9e7d7c23aba236802ca69425d78e036742884e22c7c85beb3\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/37/0f/444b73ed86b6045e2c3cc9122401f064158f035f099a0a9e64\n",
            "  Building wheel for googletransx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletransx: filename=googletransx-2.4.2-py3-none-any.whl size=15952 sha256=7dc9124ae9e1416ac235520d9d0fe669250774d1bc98085413202d07407e1527\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/35/1b/145e2fa3229232fce5e9de984e82843abf7c1a27cec582398e\n",
            "Successfully built cchardet googletransx\n",
            "Installing collected packages: dataclasses, cchardet, schedule, python-socks, elastic-transport, pycares, googletransx, elasticsearch, aiohttp_socks, aiodns, twint\n",
            "  Running setup.py develop for twint\n",
            "Successfully installed aiodns-3.2.0 aiohttp_socks-0.10.1 cchardet-2.1.7 dataclasses-0.6 elastic-transport-8.17.1 elasticsearch-8.17.2 googletransx-2.4.2 pycares-4.6.0 python-socks-2.7.1 schedule-1.2.2 twint-2.1.21\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dataclasses"
                ]
              },
              "id": "c57bbb5551874a8aac2848444df3f857"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import twint\n",
        "\n",
        "def extract_twitter_info_twint(handle):\n",
        "    try:\n",
        "        # Configure twint\n",
        "        c = twint.Config()\n",
        "        c.Username = handle\n",
        "        c.User_full = True\n",
        "        c.Store_object = True\n",
        "        c.Hide_output = True\n",
        "\n",
        "        # Run the search\n",
        "        twint.run.Lookup(c)\n",
        "\n",
        "        # Get user data\n",
        "        user = twint.output.users_list[0]\n",
        "\n",
        "        return {\n",
        "            \"username\": user.username,\n",
        "            \"displayname\": user.name,\n",
        "            \"bio\": user.bio,\n",
        "            \"location\": user.location,\n",
        "            \"followers\": user.followers,\n",
        "            \"verified\": user.verified\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Error extracting Twitter info: {str(e)}\"}"
      ],
      "metadata": {
        "id": "V7EWPDQv9z2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_twitter_info_twint(\"imVkohli\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0QPJq1F-AKy",
        "outputId": "56b135e7-bd0e-49b1-c80c-f7e4c8a3af43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'error': 'Error extracting Twitter info: Could not find the Guest token in HTML'}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests-html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsNV5ZZO-yfi",
        "outputId": "d60141f3-4472-4306-f211-4c1c50b2e992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests-html in /usr/local/lib/python3.11/dist-packages (0.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from requests-html) (2.32.3)\n",
            "Requirement already satisfied: pyquery in /usr/local/lib/python3.11/dist-packages (from requests-html) (2.0.1)\n",
            "Requirement already satisfied: fake-useragent in /usr/local/lib/python3.11/dist-packages (from requests-html) (2.1.0)\n",
            "Requirement already satisfied: parse in /usr/local/lib/python3.11/dist-packages (from requests-html) (1.20.2)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.11/dist-packages (from requests-html) (0.0.2)\n",
            "Requirement already satisfied: w3lib in /usr/local/lib/python3.11/dist-packages (from requests-html) (2.3.1)\n",
            "Requirement already satisfied: pyppeteer>=0.0.14 in /usr/local/lib/python3.11/dist-packages (from requests-html) (2.0.0)\n",
            "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (1.4.4)\n",
            "Requirement already satisfied: certifi>=2023 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (2025.1.31)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (8.6.1)\n",
            "Requirement already satisfied: pyee<12.0.0,>=11.0.0 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (11.1.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (4.67.1)\n",
            "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (1.26.20)\n",
            "Requirement already satisfied: websockets<11.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (10.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from bs4->requests-html) (4.13.3)\n",
            "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.11/dist-packages (from pyquery->requests-html) (5.3.1)\n",
            "Requirement already satisfied: cssselect>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from pyquery->requests-html) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->requests-html) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->requests-html) (3.10)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.21.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pyee<12.0.0,>=11.0.0->pyppeteer>=0.0.14->requests-html) (4.13.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4->requests-html) (2.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lxml_html_clean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilsN8m36_IBc",
        "outputId": "dec2ee0f-c8ee-492b-d773-b6478f712f2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lxml_html_clean\n",
            "  Downloading lxml_html_clean-0.4.2-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from lxml_html_clean) (5.3.1)\n",
            "Downloading lxml_html_clean-0.4.2-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: lxml_html_clean\n",
            "Successfully installed lxml_html_clean-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "\n",
        "from requests_html import AsyncHTMLSession\n",
        "\n",
        "def extract_twitter_info_request(handle):\n",
        "    try:\n",
        "        # Create a session\n",
        "        session = AsyncHTMLSession()\n",
        "\n",
        "        # Use Twitter's direct URL\n",
        "        url = f\"https://twitter.com/{handle}\"\n",
        "\n",
        "        # Send a GET request with appropriate headers\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "            'Accept-Language': 'en-US,en;q=0.9',\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml'\n",
        "        }\n",
        "\n",
        "        response = session.get(url, headers=headers)\n",
        "\n",
        "        # Render the JavaScript (this is key for modern Twitter)\n",
        "        response.html.render(timeout=30)\n",
        "\n",
        "        # Extract basic info from HTML\n",
        "        # Note: These selectors may need to be updated as Twitter changes its HTML structure\n",
        "        displayname_element = response.html.find('div[data-testid=\"UserName\"] span', first=True)\n",
        "        bio_element = response.html.find('div[data-testid=\"UserDescription\"]', first=True)\n",
        "        location_element = response.html.find('span[data-testid=\"UserLocation\"]', first=True)\n",
        "        followers_element = response.html.find('a[href*=\"/followers\"] span', first=True)\n",
        "        verified_element = response.html.find('svg[aria-label=\"Verified account\"]', first=True)\n",
        "\n",
        "        displayname = displayname_element.text if displayname_element else \"Unknown\"\n",
        "        bio = bio_element.text if bio_element else \"\"\n",
        "        location = location_element.text if location_element else \"\"\n",
        "\n",
        "        # Parse followers (might need to handle \"K\" or \"M\" suffixes)\n",
        "        followers = 0\n",
        "        if followers_element:\n",
        "            followers_text = followers_element.text\n",
        "            if 'K' in followers_text:\n",
        "                followers = float(followers_text.replace('K', '')) * 1000\n",
        "            elif 'M' in followers_text:\n",
        "                followers = float(followers_text.replace('M', '')) * 1000000\n",
        "            else:\n",
        "                followers = int(followers_text.replace(',', ''))\n",
        "\n",
        "        return {\n",
        "            \"username\": handle,\n",
        "            \"displayname\": displayname,\n",
        "            \"bio\": bio,\n",
        "            \"location\": location,\n",
        "            \"followers\": followers,\n",
        "            \"verified\": verified_element is not None\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Error extracting Twitter info: {str(e)}\"}"
      ],
      "metadata": {
        "id": "PCMX7k3V-S_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_twitter_info_request(\"imVkohli\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCLvqj5h-ofj",
        "outputId": "f658abd6-78cb-45da-e12f-13e8e87ece51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'error': \"Error extracting Twitter info: '_asyncio.Future' object has no attribute 'html'\"}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "def extract_twitter_info_soup(handle):\n",
        "    try:\n",
        "        # Use a Nitter instance (Twitter alternative frontend)\n",
        "        url = f\"https://nitter.net/{handle}\"\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "\n",
        "        response = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extract information\n",
        "        display_name = soup.select_one('.profile-card-fullname').text.strip()\n",
        "        bio = soup.select_one('.profile-bio')\n",
        "        bio_text = bio.text.strip() if bio else \"\"\n",
        "        location = soup.select_one('.profile-location')\n",
        "        location_text = location.text.strip() if location else \"\"\n",
        "\n",
        "        # Extract follower count\n",
        "        stats = soup.select('.profile-stat-num')\n",
        "        followers = 0\n",
        "        if len(stats) >= 3:\n",
        "            followers_text = stats[2].text.strip()\n",
        "            followers = int(re.sub(r'[^0-9]', '', followers_text))\n",
        "\n",
        "        # Check for verified badge\n",
        "        verified = bool(soup.select_one('.verified-icon'))\n",
        "\n",
        "        return {\n",
        "            \"username\": handle,\n",
        "            \"displayname\": display_name,\n",
        "            \"bio\": bio_text,\n",
        "            \"location\": location_text,\n",
        "            \"followers\": followers,\n",
        "            \"verified\": verified\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Error extracting Twitter info: {str(e)}\"}"
      ],
      "metadata": {
        "id": "VVD1c3oK_Yt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_twitter_info_soup(\"imVkohli\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HX-5agRNATiK",
        "outputId": "9757061b-ce47-47d8-ca86-35434d39aa72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'error': \"Error extracting Twitter info: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\"}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HxuKBzRBAY4c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}